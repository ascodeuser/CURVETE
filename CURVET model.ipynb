{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ca8d5cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:19:38.726713Z",
     "iopub.status.busy": "2023-04-30T12:19:38.725752Z",
     "iopub.status.idle": "2023-04-30T12:19:48.575362Z",
     "shell.execute_reply": "2023-04-30T12:19:48.574134Z",
     "shell.execute_reply.started": "2023-04-30T12:19:38.726626Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import os\n",
    "import glob as gb\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.densenet import DenseNet121\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Add, Dropout, Activation,\\\n",
    "               BatchNormalization, Conv2D, MaxPooling2D, GlobalMaxPooling2D,MaxPool2D\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from mlxtend.plotting import plot_confusion_matrix                  # pip install mlxtend\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score \n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from operator import truediv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba2c26a",
   "metadata": {},
   "source": [
    "## Import and prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5b89b00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:22:26.931962Z",
     "iopub.status.busy": "2023-04-30T12:22:26.930512Z",
     "iopub.status.idle": "2023-04-30T12:22:26.943292Z",
     "shell.execute_reply": "2023-04-30T12:22:26.941890Z",
     "shell.execute_reply.started": "2023-04-30T12:22:26.931917Z"
    }
   },
   "outputs": [],
   "source": [
    "def image_datagenerator(trainpath, img_height=224, img_width=224, batch_size=20):\n",
    "    \n",
    "    train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2) # set validation split\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    print(\"The data is being split into training and validation set\")\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "    trainpath,# This is the target directory\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    subset='training') # set as training data\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "\n",
    "    validation_generator = train_datagen.flow_from_directory(\n",
    "    trainpath, # same directory as training data\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    subset='validation') # set as validation data\n",
    "\n",
    "    # check the number of images in each class in the training dataset\n",
    "    No_images_per_class = []\n",
    "    training_class = []\n",
    "\n",
    "    for i in os.listdir (trainpath):         #('/content/Coursera-Content/Brain-MRI/Training'):\n",
    "        Class_name = os.listdir(os.path.join(trainpath, i))\n",
    "        No_images_per_class.append(len(Class_name))\n",
    "        training_class.append(i)\n",
    "        print('Number of images in {} = {} \\n'.format(i, len(Class_name)))\n",
    "        \n",
    "    \n",
    "    return train_generator, validation_generator, training_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58dfc65",
   "metadata": {},
   "source": [
    "### Import the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea712196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 39 images belonging to 3 classes.\n",
      "Number of images in glioma = 19 \n",
      "\n",
      "Number of images in meningioma = 8 \n",
      "\n",
      "Number of images in pituitary tumor = 12 \n",
      "\n",
      "test_classes:  {'glioma': 0, 'meningioma': 1, 'pituitary tumor': 2}\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "img_height=224\n",
    "img_width=224\n",
    "\n",
    "testpath= ('V:/brain_dataset/test_set/')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        testpath,# This is the target directory\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=1,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False, \n",
    "        seed=42) \n",
    "\n",
    "\n",
    "\n",
    "# check the number of images in each class in the test dataset\n",
    "No_images_per_class = []\n",
    "test_class = []\n",
    "\n",
    "for i in os.listdir (testpath):         #('/content/Coursera-Content/Brain-MRI/Training'):\n",
    "    Class_name = os.listdir(os.path.join(testpath, i))\n",
    "    No_images_per_class.append(len(Class_name))\n",
    "    test_class.append(i)\n",
    "    print('Number of images in {} = {} \\n'.format(i, len(Class_name)))\n",
    "        \n",
    "test_classes = test_generator.class_indices\n",
    "print('test_classes: ',test_classes)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767d898f",
   "metadata": {},
   "source": [
    "### Transfer Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94b170a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:23:04.561392Z",
     "iopub.status.busy": "2023-04-30T12:23:04.561006Z",
     "iopub.status.idle": "2023-04-30T12:23:04.569710Z",
     "shell.execute_reply": "2023-04-30T12:23:04.568319Z",
     "shell.execute_reply.started": "2023-04-30T12:23:04.561357Z"
    }
   },
   "outputs": [],
   "source": [
    "def ImageNet_model(Dataset_path, granularity,index,Level, Process):     \n",
    "\n",
    "\n",
    "    folder= granularity[Level]  \n",
    "    index= index[Level]\n",
    "\n",
    "    granularity_path= os.path.join(Dataset_path, folder)\n",
    "    train_generator, validation_generator, training_class = image_datagenerator(granularity_path)\n",
    "\n",
    "\n",
    "    base_model = DenseNet121(include_top=False, input_shape=(224, 224, 3), weights = 'imagenet')\n",
    "    x = layers.Flatten()(base_model.output)\n",
    "    x = layers.Dense(1024, activation='relu')(x)\n",
    "    predictions = layers.Dense(len(training_class), activation='softmax')(x)\n",
    "\n",
    "    base_model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    #base_model.summary()\n",
    "\n",
    "    finetuned_model, learned_weights, save_here= Training_model(base_model,train_generator,validation_generator, training_class,\n",
    "                                                         folder, index, Process ) \n",
    "\n",
    "    \n",
    "    return finetuned_model, learned_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff85dde4",
   "metadata": {},
   "source": [
    "#### CURVET (WO/CL, W/CD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "749d90b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SampleDecomp_model(Dataset_path, granularity,index,Level, Process):      \n",
    "\n",
    "    folder= granularity[Level]  \n",
    "    index= index[Level]\n",
    "    \n",
    "    granularity_path= os.path.join(Dataset_path, folder)\n",
    "    train_generator, validation_generator, training_class = image_datagenerator(granularity_path)\n",
    "    \n",
    "    ## load the learned weights after training the self-supervised model with sample decomposition using k=5\n",
    "    model = load_model('.../.../model.h5')\n",
    "    model.load_weights('.../.../weights.h5', by_name=True) \n",
    "   \n",
    "    \n",
    "    model = Model(model.input, model.layers[-2].output)\n",
    "    x = layers.Flatten()(model.output)\n",
    "    \n",
    "    new_prediction =layers.Dense(len(train_generator.class_indices),\n",
    "                             activation='softmax', name=\"new_task\")(x)\n",
    "    new_model = Model(inputs=model.input, outputs=new_prediction)\n",
    "    #new_model.summary()\n",
    "    finetuned_model, best_learnetweights , save_here = Training_model(new_model,train_generator,validation_generator, training_class, \n",
    "                                                         folder, index, Process) \n",
    "    \n",
    "    return finetuned_model, best_learnetweights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050de522",
   "metadata": {},
   "source": [
    "#### CURVET (W/CLCD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c452007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretext_model(Dataset_path, granularity,index,Level, Process):        \n",
    "    \n",
    "    \n",
    "    folder= granularity[Level]  \n",
    "    index= index[Level]\n",
    "    \n",
    "    granularity_path= os.path.join(Dataset_path, folder)\n",
    "    train_generator, validation_generator, training_class = image_datagenerator(granularity_path)\n",
    "\n",
    "    ## load the learned weights after training the model with granularity component of k=5\n",
    "    model = load_model('.../.../model.h5')\n",
    "    model.load_weights('.../.../weights.h5', by_name=True)  \n",
    "    \n",
    "    ## load the learned weights after training the model with granularity component of k=10\n",
    "    #model = load_model('.../.../model.h5')\n",
    "    #model.load_weights('.../.../weights.h5', by_name=True) \n",
    "    \n",
    "    model = Model(model.input, model.layers[-2].output)\n",
    "    x = layers.Flatten()(model.output)\n",
    "    \n",
    "    new_prediction =layers.Dense(len(train_generator.class_indices),\n",
    "                             activation='softmax', name=\"new_task\")(x)\n",
    "    new_model = Model(inputs=model.input, outputs=new_prediction)\n",
    "    #new_model.summary()\n",
    "    finetuned_model, best_learnetweights , save_here = Training_model(new_model,train_generator,validation_generator, training_class, \n",
    "                                                         folder, index, Process) \n",
    "    \n",
    "    return finetuned_model, best_learnetweights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40fd5d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CURVET(granularity, index, Level, Process):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function manages the training of CURVET model using different levels of granularity\n",
    "    and applies different strategies based on the specified process. The function iteratively \n",
    "    refines the model through both descending and ascending levels of granularity, leveraging data \n",
    "    generators for training and validation.\n",
    "\n",
    "    Parameters:\n",
    "    granularity (list): A list of granularity levels used for training sorted in descending order.\n",
    "    index (list): A list of indices corresponding to the granularity levels.\n",
    "    Level (int): The current granularity level being processed.\n",
    "    process (str): The type of training process to be executed, determining which model to use.\n",
    "\n",
    "    Returns:\n",
    "    None: The function performs training and does not return a value. \n",
    "    It saves the trained model and weights to the specified directory during the process.\n",
    "    \"\"\"\n",
    "    \n",
    "    if Process == 'CURVET (W_CLCD)':\n",
    "        finetuned_model, learned_weights = pretext_model(Dataset_path, granularity, index, Level, Process)\n",
    "    elif Process == 'CURVET (WO_CL W_CD)':\n",
    "        finetuned_model, learned_weights = SampleDecomp_model(Dataset_path, granularity, index, Level, Process)\n",
    "    elif Process == 'CURVET (WO_CLCD)':\n",
    "        finetuned_model, learned_weights = ImageNet_model(Dataset_path, granularity, index, Level, Process)\n",
    "    \n",
    "    num_iter = 0\n",
    "    max_iterations = 20\n",
    "\n",
    "    # Repeat the process for a specified number of iterations\n",
    "    while num_iter < max_iterations:\n",
    "        num_iter += 1\n",
    "\n",
    "        # Descending process\n",
    "        for i in range(len(granularity) - 1):  # [g_4,...,g_1]\n",
    "            path_iter = os.path.join(save_to_dir,Process, str(num_iter))\n",
    "            if not os.path.exists(path_iter):\n",
    "                os.makedirs(path_iter)\n",
    "\n",
    "            Level += 1          \n",
    "            next_level = granularity[Level]\n",
    "            next_index = index[Level]\n",
    "            print('Move towards the lower level.... ', next_level, 'next_index=', next_index)\n",
    "            granularity_path = os.path.join(Dataset_path, next_level)\n",
    "\n",
    "            train_generator, validation_generator, training_class = image_datagenerator(granularity_path)\n",
    "\n",
    "            finetuned_model, learned_weights, save_here = TransferLearning(\n",
    "                finetuned_model, learned_weights, train_generator,\n",
    "                validation_generator, training_class, next_level,\n",
    "                next_index, Process=path_iter\n",
    "            )  \n",
    "\n",
    "        # Backward process\n",
    "        beta = Level  # backward from the lowest granularity to the highest granularity\n",
    "        num_iter+=1\n",
    "        \n",
    "        for j in range(len(granularity) - 1):  \n",
    "            \n",
    "            path_iter = os.path.join(save_to_dir,Process, str(num_iter))\n",
    "            if not os.path.exists(path_iter):\n",
    "                os.makedirs(path_iter)\n",
    "\n",
    "            beta -= 1    \n",
    "            backward_level = granularity[beta]\n",
    "            backward_index = index[beta]\n",
    "            print('Move towards the higher level....', backward_level, 'backward_index=', backward_index)\n",
    "\n",
    "            granularity_path = os.path.join(Dataset_path, backward_level)\n",
    "            train_generator, validation_generator, training_class = image_datagenerator(granularity_path)\n",
    "\n",
    "            finetuned_model, learned_weights, save_here = TransferLearning(\n",
    "                finetuned_model, learned_weights, train_generator,\n",
    "                validation_generator, training_class, backward_level,\n",
    "                backward_index, Process=path_iter\n",
    "            )\n",
    "        \n",
    "        Level = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1ebba33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TransferLearning(TransfereLearned_Model, learned_weights, train_generator, validation_generator,\n",
    "                      training_class, next_level, index, Process):\n",
    "    \"\"\"\n",
    "    This function modifies a pre-trained model to adapt it for a new classification task.\n",
    "    It loads the learned weights from the previous level, replaces the output layer with a new dense layer for the \n",
    "    current training class, and trains the model using provided data generators.\n",
    "    Returns:\n",
    "    The fine-tuned model, the learned weights, and the path to save the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the learned weights into the pre-trained model\n",
    "    TransfereLearned_Model.load_weights(learned_weights)  # Load weights from the specified path\n",
    "\n",
    "    # Create a new model that outputs from the second-to-last layer of the pre-trained model\n",
    "    TransfereLearned_Model = Model(TransfereLearned_Model.input, TransfereLearned_Model.layers[-2].output)\n",
    "\n",
    "    # Add a new classification output layer for the new task\n",
    "    new_prediction = layers.Dense(len(training_class), activation='softmax', name=\"new_task\")(TransfereLearned_Model.output)\n",
    "\n",
    "    TransfereLearned_Model = Model(inputs=TransfereLearned_Model.input, outputs=new_prediction)\n",
    "\n",
    "    # Train the model using the provided training and validation generators\n",
    "    finetuned_model, learned_weights, save_here = Training_model(\n",
    "        TransfereLearned_Model, train_generator, validation_generator,\n",
    "        training_class, next_level, index, Process\n",
    "    )\n",
    "\n",
    "    return finetuned_model, learned_weights, save_here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "866d1cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Training_model(finetuning_model, train_generator, validation_generator, training_class, folder_name, index, Process):\n",
    "    \"\"\"\n",
    "    This function handles the training process for a fine-tuned model.\n",
    "\n",
    "    Parameters:\n",
    "    folder (str): The current folder level for saving weights.\n",
    "    index (int): The index corresponding to the current training iteration.\n",
    "    Process (str): A string indicating the type of training process.\n",
    "\n",
    "    Returns:\n",
    "    The trained model and the directory for saving outputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the directory to save the weights based on the process type\n",
    "    save_here = os.path.join(save_to_dir, Process)\n",
    "    \n",
    "    # Create the directory if it does not exist\n",
    "    if not os.path.exists(save_here):\n",
    "        os.makedirs(save_here)\n",
    "            \n",
    "    learned_weights = os.path.join(save_here, 'weights_' + folder_name + '.h5')\n",
    "   \n",
    "    # Define checkpoint for saving the best model weights\n",
    "    checkpoint = ModelCheckpoint(filepath=learned_weights,\n",
    "                                 monitor='val_accuracy', save_best_only=True,\n",
    "                                 save_weights_only=True, \n",
    "                                 mode='max', verbose=1)\n",
    "    \n",
    "    # Early stopping callback to prevent overfitting\n",
    "    earlystop = EarlyStopping(monitor=\"val_accuracy\", patience=8, mode=\"auto\")\n",
    "\n",
    "    # Learning rate scheduler to adjust learning rate during training\n",
    "    def lr_scheduler(epoch):\n",
    "        initial_lrate = 0.001\n",
    "        drop = 0.85\n",
    "        epochs_drop = 15.0\n",
    "        lrate = initial_lrate * math.pow(drop, math.floor((1 + epoch) / epochs_drop))\n",
    "        return lrate\n",
    "  \n",
    "    lrscheduler = LearningRateScheduler(lr_scheduler)\n",
    "    callbacks = [checkpoint, earlystop, lrscheduler]\n",
    "\n",
    "    batch_size = 50\n",
    "    \n",
    "    # Display the number of training and validation samples\n",
    "    print(f\"Number of training samples: {train_generator.samples}\")\n",
    "    print(f\"Number of validation samples: {validation_generator.samples}\")\n",
    "\n",
    "    # Calculate steps per epoch based on the size of training data and batch size\n",
    "    steps_per_epoch = max(1, train_generator.samples // batch_size)\n",
    "    validation_steps = max(1, validation_generator.samples // batch_size)\n",
    "\n",
    "    print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "    print(f\"Validation steps: {validation_steps}\")\n",
    "    \n",
    "    # Compile the model with an optimizer, loss function, and metrics\n",
    "    finetuning_model.compile(optimizer=SGD(), loss=\"mse\", metrics=[\"accuracy\"])\n",
    "\n",
    "    # Train the model using the training and validation data generators\n",
    "    history = finetuning_model.fit(train_generator,\n",
    "                                   steps_per_epoch=steps_per_epoch,\n",
    "                                   validation_data=validation_generator,\n",
    "                                   validation_steps=validation_steps, \n",
    "                                   epochs=50,\n",
    "                                   callbacks=callbacks, \n",
    "                                   verbose=1, \n",
    "                                   shuffle=True)\n",
    "    \n",
    "    finetuning_model.save(os.path.join(save_here, 'model_' + folder_name + '.h5'))\n",
    "    \n",
    "    # Get predictions and generate confusion matrix\n",
    "    y_true, y_predict = model_prediction(finetuning_model, learned_weights, save_here)\n",
    "    ConfusionMatrix(y_true, y_predict, save_here, folder_name, index)\n",
    "\n",
    "    return finetuning_model, learned_weights, save_here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80ab413",
   "metadata": {},
   "source": [
    "#####  Make a prediction on a test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3519af4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:24:03.527154Z",
     "iopub.status.busy": "2023-04-30T12:24:03.526422Z",
     "iopub.status.idle": "2023-04-30T12:24:03.534457Z",
     "shell.execute_reply": "2023-04-30T12:24:03.533373Z",
     "shell.execute_reply.started": "2023-04-30T12:24:03.527114Z"
    }
   },
   "outputs": [],
   "source": [
    "def model_prediction( model, learned_weights, save_here, batch_size=1):\n",
    "    x_test , y_test = [] , []\n",
    "    \n",
    "    # Iterate over the test generator to collect the test samples and true labels\n",
    "    for i in range(test_generator.n//1):\n",
    "        a , b = test_generator.next()\n",
    "        x_test.extend(a)\n",
    "        y_test.extend(b)\n",
    "    y_test= np.array(y_test)       # Convert labels to numpy array for easier processing\n",
    "\n",
    "    # Predict the output\n",
    "    STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
    "    test_generator.reset()\n",
    "\n",
    "\n",
    "    # # Load the best learned weights from the directory\n",
    "    model.load_weights(os.path.join(save_here,learned_weights) )       \n",
    "    print('Make a prediction on a test set:')\n",
    "    y_test_pred= model.predict(test_generator,steps=STEP_SIZE_TEST,verbose=1)\n",
    "    y_prediction = np.argmax(y_test_pred, axis=1)## predicted_class_indices\n",
    "\n",
    "    # ground truth labels\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "    return y_true, y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84688a3c",
   "metadata": {},
   "source": [
    "##### Evaluate the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41a7e03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConfusionMatrix(y_true, y_predict, save_here, folder_name, index):  \n",
    "    \n",
    "    # Refine predictions for levels other than 'k_1' using error-correction\n",
    "    if folder_name != 'k_1':\n",
    "        correct_prediction = [i // index for i in y_predict]\n",
    "        y_predict = np.array(correct_prediction)\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_predict)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    fig, ax = plot_confusion_matrix(conf_mat=cm, figsize=(6, 6),\n",
    "                                    colorbar=False,\n",
    "                                    show_absolute=True,\n",
    "                                    show_normed=False,\n",
    "                                    class_names=test_classes, cmap=\"Blues\")\n",
    "    plt.savefig(os.path.join(save_here, 'ConfusionMatrix_' + folder_name + '.png'))\n",
    "    plt.show()\n",
    "\n",
    "    # Classification report\n",
    "    print(classification_report(y_true, y_predict, target_names=test_classes, digits=4))\n",
    "\n",
    "    # Print overall accuracy\n",
    "    print('Overall accuracy= ', accuracy_score(y_true, y_predict))\n",
    "\n",
    "    # Precision and recall\n",
    "    tp = np.diag(cm)  # True positives from confusion matrix\n",
    "    precision = np.nan_to_num(np.diag(cm) / np.sum(cm, axis=0))  # Avoid div/0\n",
    "    recall = np.nan_to_num(np.diag(cm) / np.sum(cm, axis=1))\n",
    "\n",
    "    print(f'\\nPrecision: {precision}\\nRecall: {recall}')\n",
    "\n",
    "    # Calculate F1 score\n",
    "    F1_score = 2 * ((precision * recall) / np.clip(precision + recall, a_min=1e-10, a_max=None))  # Avoid div/0\n",
    "    print('F1_score= ', F1_score)\n",
    "\n",
    "    # Overall precision, recall, and F1\n",
    "    PR = np.mean(precision)\n",
    "    RE = np.mean(recall)\n",
    "    F1 = 2 * ((PR * RE) / np.clip(PR + RE, a_min=1e-10, a_max=None))\n",
    "    \n",
    "    print(f'Overall Precision= {PR}\\nOverall Recall= {RE}\\nOverall F1_score= {F1}')\n",
    "    \n",
    "    # Save results to CSV\n",
    "    report= {'g':[folder_name], 'ACC':[accuracy_score(y_true, y_predict)], \n",
    "             'precision':[PR], 'recall':[RE], 'F1_score':[F1]}             \n",
    "    \n",
    "    df = pd.DataFrame(report) \n",
    "\n",
    "    df.to_csv(os.path.join(save_here, 'confusion_reports.csv'), mode='a', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b365cef4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:23:01.382477Z",
     "iopub.status.busy": "2023-04-30T12:23:01.381299Z",
     "iopub.status.idle": "2023-04-30T12:23:01.393921Z",
     "shell.execute_reply": "2023-04-30T12:23:01.392525Z",
     "shell.execute_reply.started": "2023-04-30T12:23:01.382422Z"
    }
   },
   "outputs": [],
   "source": [
    "class The_proccess():\n",
    "  \n",
    "    def first(self):\n",
    "        print(\"Transfer learning with a pretrained-imagenet\")\n",
    "        Level=-1\n",
    "        ImageNet_model(Dataset_path, G, index, Level, Process= 'pretrained-imagenet')\n",
    "        \n",
    "    def second(self):\n",
    "        print(\"Training CURVET (WO/CLCD)\")\n",
    "        Level=0\n",
    "        CURVET (G, index, Level, Process='CURVET (WO_CLCD)')\n",
    "\n",
    "    def third(self):\n",
    "        print(\"Training CURVET (WO/CL, W/CD)\")\n",
    "        Level=0\n",
    "        CURVET (G, index, Level, Process='CURVET (WO_CL W_CD)')\n",
    "        \n",
    "    def forth(self):\n",
    "        print(\"Training CURVET (W/CLCD)\")\n",
    "        Level=0\n",
    "        CURVET (G, index, Level, Process='CURVET (W_CLCD)')\n",
    "        \n",
    "        \n",
    "    def __init__(self):\n",
    "        self.method = input(\"Which process do you want to use? \\n\\n 1) pretrained-imagenet. \\n\\n 2) CURVET (WO/CLCD). \\n\\n 3) CURVET (WO/CL, W/CD). \\n\\n 4) CURVET (W/CLCD) \\n\\n Please enter the corresponding number and hit enter >>>>> \")\n",
    "\n",
    "        if self.method == str(1):\n",
    "            self.first()\n",
    "        elif self.method == str(2):\n",
    "            self.second()\n",
    "        elif self.method == str(3):\n",
    "            self.third()\n",
    "        elif self.method == str(4):\n",
    "            self.forth()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a5aaced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Granularity Levels (G): ['g_5', 'g_4', 'g_3', 'g_2', 'g_1']\n",
      "Corresponding Indices (index): [5, 4, 3, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "# Paths to the dataset and the directory where results will be saved\n",
    "\n",
    "Dataset_path = 'V:/....../brain dataset/'  \n",
    "save_to_dir = 'C:/....../granularity datasets/' \n",
    "\n",
    "# Determine the decomposition granularity by listing all the folders in the dataset path\n",
    "# Each folder represents a granularity level (e.g., g1, g2, ..., g9_max)\n",
    "decomposition_granularity = os.listdir(Dataset_path)\n",
    "\n",
    "\n",
    "# Sort the granularity levels in descending order (from g_max to g_min)\n",
    "# Example: [g5, g4, ..., g1]\n",
    "decomposition_granularity.sort(reverse=True)\n",
    "\n",
    "# Initialize lists to store the granularity levels (G) and their corresponding indices\n",
    "G = []      \n",
    "index = []  \n",
    "\n",
    "# Loop through the sorted granularity levels and assign them to the lists\n",
    "for i, folder in enumerate(decomposition_granularity, 1):\n",
    "    G.append(folder)      \n",
    "    index.append(i)       \n",
    "\n",
    "# Sort the indices in descending order to maintain the order with G\n",
    "index.sort(reverse=True)\n",
    "\n",
    "# Display the sorted granularity levels and corresponding indices\n",
    "print('Granularity Levels (G):', G)\n",
    "print('Corresponding Indices (index):', index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acb0a9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CURVETE = The_proccess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa9333f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f801aa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bfa5b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc62762",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
