{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca8d5cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:19:38.726713Z",
     "iopub.status.busy": "2023-04-30T12:19:38.725752Z",
     "iopub.status.idle": "2023-04-30T12:19:48.575362Z",
     "shell.execute_reply": "2023-04-30T12:19:48.574134Z",
     "shell.execute_reply.started": "2023-04-30T12:19:38.726626Z"
    },
    "id": "5ca8d5cb"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import os\n",
    "import glob as gb\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.densenet import DenseNet121\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Add, Dropout, Activation,\\\n",
    "               BatchNormalization, Conv2D, MaxPooling2D, GlobalMaxPooling2D,MaxPool2D\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from mlxtend.plotting import plot_confusion_matrix                  # pip install mlxtend\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from operator import truediv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba2c26a",
   "metadata": {
    "id": "fba2c26a"
   },
   "source": [
    "## Import and prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b89b00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:22:26.931962Z",
     "iopub.status.busy": "2023-04-30T12:22:26.930512Z",
     "iopub.status.idle": "2023-04-30T12:22:26.943292Z",
     "shell.execute_reply": "2023-04-30T12:22:26.941890Z",
     "shell.execute_reply.started": "2023-04-30T12:22:26.931917Z"
    },
    "id": "d5b89b00"
   },
   "outputs": [],
   "source": [
    "def image_datagenerator(trainpath, img_height=224, img_width=224, batch_size=20):\n",
    "\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2) # set validation split\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    print(\"The data is being split into training and validation set\")\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "    trainpath,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    subset='training') # set as training data\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "\n",
    "    validation_generator = train_datagen.flow_from_directory(\n",
    "    trainpath, # same directory as training data\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    subset='validation') # set as validation data\n",
    "\n",
    "    # check the number of images in each class in the training dataset\n",
    "    No_images_per_class = []\n",
    "    training_class = []\n",
    "\n",
    "    for i in os.listdir (trainpath):         \n",
    "        Class_name = os.listdir(os.path.join(trainpath, i))\n",
    "        No_images_per_class.append(len(Class_name))\n",
    "        training_class.append(i)\n",
    "        print('Number of images in {} = {} \\n'.format(i, len(Class_name)))\n",
    "\n",
    "\n",
    "    return train_generator, validation_generator, training_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58dfc65",
   "metadata": {
    "id": "d58dfc65"
   },
   "source": [
    "### Import the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea712196",
   "metadata": {
    "id": "ea712196",
    "outputId": "616914a7-9b19-4cf3-e09e-ee7a06d5ba09"
   },
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "img_height=224\n",
    "img_width=224\n",
    "\n",
    "testpath= ('................./test_set/')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        testpath,# This is the target directory\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=1,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False,\n",
    "        seed=42)\n",
    "\n",
    "# check the number of images in each class in the test dataset\n",
    "No_images_per_class = []\n",
    "test_class = []\n",
    "\n",
    "for i in os.listdir (testpath):         \n",
    "    Class_name = os.listdir(os.path.join(testpath, i))\n",
    "    No_images_per_class.append(len(Class_name))\n",
    "    test_class.append(i)\n",
    "    print('Number of images in {} = {} \\n'.format(i, len(Class_name)))\n",
    "\n",
    "test_classes = test_generator.class_indices\n",
    "print('test_classes: ',test_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b365cef4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:23:01.382477Z",
     "iopub.status.busy": "2023-04-30T12:23:01.381299Z",
     "iopub.status.idle": "2023-04-30T12:23:01.393921Z",
     "shell.execute_reply": "2023-04-30T12:23:01.392525Z",
     "shell.execute_reply.started": "2023-04-30T12:23:01.382422Z"
    },
    "id": "b365cef4"
   },
   "outputs": [],
   "source": [
    "class The_proccess():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.method = input(\"Which process do you want to use? \\n\\n 1) pretrained-imagenet. \\n\\n 2) CLOG-CD. \\n\\n 3) Without curriculum learning in pretext model. \\n\\n 4) CURVETE \\n\\n Please enter the corresponding number and hit enter >>>>> \")\n",
    "\n",
    "        if self.method == str(1):\n",
    "            self.first()\n",
    "        elif self.method == str(2):\n",
    "            self.second()\n",
    "        elif self.method == str(3):\n",
    "            self.third()\n",
    "        elif self.method == str(4):\n",
    "            self.forth()\n",
    "\n",
    "    def first(self):\n",
    "        print(\"Transfer learning with a pretrained-imagenet \\n\")\n",
    "        Level=-1\n",
    "        ImageNet_model(Dataset_path, G, index, Level, Process= 'pretrained-imagenet')\n",
    "\n",
    "    def second(self):\n",
    "        print(\"Training CLOG-CD \\n\")\n",
    "        Level=0\n",
    "        CLOGCD (Dataset_path, G, index, Level, Process='CLOG-CD')\n",
    "\n",
    "    def third(self):\n",
    "        print(\"Training CURVET (WO/CL, W/SD) \\n\")\n",
    "        Level=0\n",
    "        CURVETE_WOCL_WSD (Dataset_path, pseudo_labels_path, G, index, Level, Process='CURVET (WOCL_WSD)')\n",
    "\n",
    "    def forth(self):\n",
    "        print(\"Training CURVET model\\n\")\n",
    "        Level=0\n",
    "        CURVETE (Dataset_path, pseudo_labels_path, G, index, Level, Process='CURVETE_model')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767d898f",
   "metadata": {
    "id": "767d898f"
   },
   "source": [
    "### Traditional Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0065cf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImageNet_model(Dataset_path, granularity, index, Level, Process):\n",
    "    \"\"\"\n",
    "    Trains a DenseNet121-based model on the dataset at the given granularity level.\n",
    "\n",
    "    Args:\n",
    "        Dataset_path (str): Path to the dataset (labelled or pseudo-labelled).\n",
    "        granularity (list): List of granularity levels, e.g. ['g_5','g_4','g_3','g_2','g_1'].\n",
    "        index (list): List of decomposition indices, matching the granularity list, e.g. [5,4,3,2,1].\n",
    "        Level (int): Current granularity level index (0 = g_5).\n",
    "        Process (str): Label for saving/tracking the training process.\n",
    "    \"\"\"\n",
    "\n",
    "    # Select granularity folder (e.g., g_5, g_4, etc.)\n",
    "    folder = granularity[Level]\n",
    "    index_value = index[Level]   # index associated with this granularity level\n",
    "    granularity_path = os.path.join(Dataset_path, folder)\n",
    "\n",
    "    print(f\"\\n Training at granularity {folder} (index={index_value}) for process {Process}\\n\")\n",
    "\n",
    "    # Prepare data generators for training and validation\n",
    "    train_generator, validation_generator, training_class = image_datagenerator(granularity_path)\n",
    "\n",
    "    # Build the DenseNet121-based model\n",
    "    base_model = DenseNet121(include_top=False, input_shape=(224, 224, 3), weights='imagenet')\n",
    "    x = layers.Flatten()(base_model.output)\n",
    "    x = layers.Dense(1024, activation='relu')(x)\n",
    "    predictions = layers.Dense(len(training_class), activation='softmax')(x)\n",
    "    base_model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # Train the model using the given data\n",
    "    finetuned_model, learned_weights, save_here = Training_model(\n",
    "        base_model, train_generator, validation_generator, training_class,\n",
    "        folder, index_value, Process\n",
    "    )\n",
    "\n",
    "    return finetuned_model, learned_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbfff7c",
   "metadata": {
    "id": "7dbfff7c"
   },
   "source": [
    "### Transfer previous knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e07c94f",
   "metadata": {
    "id": "9e07c94f"
   },
   "outputs": [],
   "source": [
    "def TransferLearning(TransfereLearned_Model, learned_weights, train_generator, validation_generator,\n",
    "                      training_class, next_level, index, Process):\n",
    "    \"\"\"\n",
    "    This function modifies a pre-trained model to adapt it for a new classification task.\n",
    "    It loads the learned weights from the previous level, replaces the output layer with a new dense layer for the\n",
    "    current training class, and trains the model using provided data generators.\n",
    "    Returns:\n",
    "    The fine-tuned model, the learned weights, and the path to save the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the learned weights into the pre-trained model\n",
    "    TransfereLearned_Model.load_weights(learned_weights)  # Load weights from the specified path\n",
    "\n",
    "    # Create a new model that outputs from the second-to-last layer of the pre-trained model\n",
    "    TransfereLearned_Model = Model(TransfereLearned_Model.input, TransfereLearned_Model.layers[-2].output)\n",
    "\n",
    "    # Add a new classification output layer for the new task\n",
    "    new_prediction = layers.Dense(len(training_class), activation='softmax', name=\"new_task\")(TransfereLearned_Model.output)\n",
    "\n",
    "    TransfereLearned_Model = Model(inputs=TransfereLearned_Model.input, outputs=new_prediction)\n",
    "\n",
    "    # Train the model using the provided training and validation generators\n",
    "    finetuned_model, learned_weights, save_here = Training_model(\n",
    "        TransfereLearned_Model, train_generator, validation_generator,\n",
    "        training_class, next_level, index, Process\n",
    "    )\n",
    "\n",
    "    return finetuned_model, learned_weights, save_here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615ac6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CurriculumDecomp(model, weights, Dataset_path, granularity, index, Level, Process, skip_first=False):\n",
    "    \"\"\"\n",
    "    Curriculum learning with class decomposition across different granularity levels.\n",
    "    - If skip_first=True: used by CLOG-CD.\n",
    "    - If skip_first=False: used by CURVETE_WOCL_WSD downstream.\n",
    "    \"\"\"\n",
    "    model.load_weights(weights)\n",
    "    num_iter = 0\n",
    "    delta = 1\n",
    "\n",
    "    curr = Level\n",
    "    max_iterations = 20\n",
    "    \n",
    "    while num_iter < max_iterations:\n",
    "        num_iter += 1\n",
    "\n",
    "        # Forward curriculum\n",
    "        forward_indices = list(range(curr, len(granularity)))\n",
    "        if skip_first and forward_indices:\n",
    "            forward_indices = forward_indices[1:]  # drop the current level (e.g., skip g_5 for CLOG-CD)\n",
    "\n",
    "        for idx in forward_indices:\n",
    "            path_iter = os.path.join(save_to_dir, Process, f\"{num_iter}\")\n",
    "            os.makedirs(path_iter, exist_ok=True)\n",
    "\n",
    "            next_level = granularity[idx]\n",
    "            next_index = index[idx]\n",
    "            print(f'Forward → , granularity={next_level}, index={next_index}')\n",
    "\n",
    "            granularity_path = os.path.join(Dataset_path, next_level)\n",
    "            train_generator, validation_generator, training_class = image_datagenerator(granularity_path)\n",
    "\n",
    "            model, weights, _ = TransferLearning(\n",
    "                model, weights, train_generator, validation_generator,\n",
    "                training_class, next_level, next_index, Process=path_iter\n",
    "            )\n",
    "\n",
    "        # Update current pointer to the last index we used (if any)\n",
    "        if forward_indices:\n",
    "            curr = forward_indices[-1]\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # Backward curriculum\n",
    "        num_iter += 1\n",
    "        backward_indices = list(range(curr - 1, -1, -1))  # go from previous level down to 0\n",
    "\n",
    "        for idx in backward_indices:\n",
    "            path_iter = os.path.join(save_to_dir, Process, f\"{num_iter}\")\n",
    "            os.makedirs(path_iter, exist_ok=True)\n",
    "\n",
    "            backward_level = granularity[idx]\n",
    "            backward_index = index[idx]\n",
    "            print(f'Backward ← , granularity={backward_level}, index={backward_index}')\n",
    "\n",
    "            granularity_path = os.path.join(Dataset_path, backward_level)\n",
    "            train_generator, validation_generator, training_class = image_datagenerator(granularity_path)\n",
    "\n",
    "            model, weights, _ = TransferLearning(\n",
    "                model, weights, train_generator, validation_generator,\n",
    "                training_class, backward_level, backward_index, Process=path_iter\n",
    "            )\n",
    "\n",
    "        # Reset for next iteration \n",
    "        curr = 0\n",
    "\n",
    "    return model, weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ce9384",
   "metadata": {
    "id": "31ce9384"
   },
   "source": [
    "### CLOG-CD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a1835f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CLOGCD(Dataset_path, granularity, index, Level, Process):\n",
    "    \"\"\"\n",
    "    Supervised training pipeline: uses ImageNet pre-trained weights.\n",
    "    Applies curriculum learning and class decomposition on real labeled data.\n",
    "    \"\"\"\n",
    "    model, weights = ImageNet_model(Dataset_path, granularity, index, Level, Process)  # trains starting from g_5\n",
    "    # Now continue curriculum starting at g_4\n",
    "    model, weights = CurriculumDecomp(model, weights, Dataset_path,\n",
    "                                      granularity, index, Level, Process,\n",
    "                                      skip_first=True)   # means skip g_5 and starts training from g_4\n",
    "    return model, weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86c3f54",
   "metadata": {
    "id": "a86c3f54"
   },
   "source": [
    "### CURVET (WO/CL, W/SD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e7e0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pretext_SampleDecomp(pseudo_labels_path, granularity, index, Level, Process):\n",
    "    \"\"\"\n",
    "    Pretext training using self-supervised sample decomposition ONLY.\n",
    "    No curriculum learning applied in this stage.\n",
    "    Always starts from g_5 (Level=0).\n",
    "    \"\"\"\n",
    "\n",
    "    #Level = 0   # g_5\n",
    "\n",
    "    model, weights = ImageNet_model(\n",
    "        Dataset_path=pseudo_labels_path,\n",
    "        granularity=granularity,\n",
    "        index=index,\n",
    "        Level=Level,\n",
    "        Process=Process + \"_pretext_noCL\"\n",
    "    )\n",
    "    return model, weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb159ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CURVETE_WOCL_WSD(Dataset_path, pseudo_labels_path, granularity, index, Level, Process):\n",
    "    \"\"\"\n",
    "    CURVETE (WO/CL, W/SD):\n",
    "\n",
    "    Stage 1: Pretext task using pseudo-labeled data with sample decomposition only.\n",
    "             Curriculum learning is NOT used in this stage.\n",
    "\n",
    "    Stage 2: Downstream fine-tuning using class decomposition + curriculum learning\n",
    "             with very limited real labeled data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Stage 1: Self-supervised sample decomposition (no curriculum)\n",
    "    print(\"\\n Starting Stage 1: Pretext training (Sample Decomposition only, No Curriculum) \\n\")\n",
    "    model, weights = Pretext_SampleDecomp(pseudo_labels_path, granularity, index, Level, Process)\n",
    "\n",
    "    print(\"Starting Stage 2: Downstream training with Curriculum Learning + Class Decomposition \\n\")\n",
    "    # Start at g_5, index=5 (Level should point to g_5, typically Level=0)\n",
    "    model, weights = CurriculumDecomp(model, weights, Dataset_path,\n",
    "                                      granularity, index, Level, Process + \"_downstream\",\n",
    "                                      skip_first=False)\n",
    "    return model, weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff85dde4",
   "metadata": {
    "id": "ff85dde4"
   },
   "source": [
    "### CURVETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0212fda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainingPseudoLabels(pseudo_labels_path, granularity, index, Level, Process):\n",
    "    \"\"\"\n",
    "    CURVETE Stage 1: Pretext training on pseudo-labeled data (self-supervised).\n",
    "    Now uses curriculum learning on pseudo-labelled data (g_5 → g_1).\n",
    "    \"\"\"\n",
    "    # Stage 1 starts at g_5 using pseudo-labels\n",
    "    model, weights = ImageNet_model(\n",
    "        Dataset_path=pseudo_labels_path,\n",
    "        granularity=granularity,   \n",
    "        index=index,\n",
    "        Level=Level,               \n",
    "        Process=Process + \"_pretext\"\n",
    "    )\n",
    "\n",
    "    # Continue curriculum: g_4 → g_1\n",
    "    model, weights = CurriculumDecomp(\n",
    "        model, weights, pseudo_labels_path,\n",
    "        granularity, index, Level, Process + \"_pretext\",\n",
    "        skip_first=True   # Skip g_5 since it was already trained above\n",
    "    )\n",
    "\n",
    "    return model, weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c452007",
   "metadata": {
    "id": "2c452007"
   },
   "outputs": [],
   "source": [
    "def CURVETE(Dataset_path, pseudo_labels_path, granularity, index, Level, Process):\n",
    "    \"\"\"\n",
    "    CURVETE: Self-supervised pretext training (Stage 1) followed by supervised fine-tuning (Stage 2).\n",
    "\n",
    "    Stage 1: Uses pseudo-labeled data to pretrain the model based curriculum learning and sample decomposition.\n",
    "    Stage 2: Fine-tunes the pretrained model on a small set of labelled data. Here, the model also trained based\n",
    "    on curriculum learning combined with different granularity of decomposition.\n",
    "    \"\"\"\n",
    "\n",
    "    # Stage 1 – Self-supervised pretext learning\n",
    "    print(\"\\n Starting Stage 1: Pretext training on pseudo-labels\")\n",
    "    model, weights = TrainingPseudoLabels(pseudo_labels_path, granularity, index, Level, Process + \"_pretext\")\n",
    "\n",
    "    # Stage 2 – Downstream task with real labels\n",
    "    print(\"\\n Starting Stage 2: Fine-tuning on labeled downstream task\")\n",
    "    # Start at g_5, index=5 (Level should point to g_5, typically Level=0)\n",
    "    model, weights = CurriculumDecomp(model, weights, Dataset_path,\n",
    "                                      granularity, index, Level, Process + \"_downstream\",\n",
    "                                      skip_first=False)\n",
    "\n",
    "    return model, weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7591568",
   "metadata": {
    "id": "c7591568"
   },
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9e8f51",
   "metadata": {
    "id": "3a9e8f51"
   },
   "outputs": [],
   "source": [
    "def Training_model(finetuning_model, train_generator, validation_generator, training_class, folder, index, Process):\n",
    "    \"\"\"\n",
    "    This function handles the training process for a fine-tuned model.\n",
    "\n",
    "    Parameters:\n",
    "    folder (str): The current folder level for saving weights.\n",
    "    index (int): The index corresponding to the current training iteration.\n",
    "    Process (str): A string indicating the type of training process.\n",
    "\n",
    "    Returns:\n",
    "    The trained model and the directory for saving outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the directory to save the weights based on the process type\n",
    "    save_here = os.path.join(save_to_dir, Process)\n",
    "\n",
    "    # Create the directory if it does not exist\n",
    "    if not os.path.exists(save_here):\n",
    "        os.makedirs(save_here)\n",
    "\n",
    "#    learned_weights = os.path.join(save_here, 'weights_' + folder + '.h5')\n",
    "    learned_weights = os.path.join(save_here, f'weights_{folder}.h5')\n",
    "\n",
    "\n",
    "    # Define checkpoint for saving the best model weights\n",
    "    checkpoint = ModelCheckpoint(filepath=learned_weights,\n",
    "                                 monitor='val_accuracy', \n",
    "                                 save_best_only=True,\n",
    "                                 save_weights_only=True,\n",
    "                                 mode='max', verbose=1)\n",
    "\n",
    "    # Early stopping callback to prevent overfitting\n",
    "    earlystop = EarlyStopping(monitor=\"val_accuracy\", patience=8, mode=\"auto\")\n",
    "\n",
    "    # Learning rate scheduler to adjust learning rate during training\n",
    "    def lr_scheduler(epoch):\n",
    "        initial_lrate = 0.001\n",
    "        drop = 0.85\n",
    "        epochs_drop = 15.0\n",
    "        lrate = initial_lrate * math.pow(drop, math.floor((1 + epoch) / epochs_drop))\n",
    "        return lrate\n",
    "\n",
    "    lrscheduler = LearningRateScheduler(lr_scheduler)\n",
    "    callbacks = [checkpoint, earlystop, lrscheduler]\n",
    "\n",
    "    batch_size = 50\n",
    "\n",
    "    # Display the number of training and validation samples\n",
    "    print(f\"Number of training samples: {train_generator.samples}\")\n",
    "    print(f\"Number of validation samples: {validation_generator.samples}\")\n",
    "\n",
    "    # Calculate steps per epoch based on the size of training data and batch size\n",
    "    steps_per_epoch = max(1, train_generator.samples // batch_size)\n",
    "    validation_steps = max(1, validation_generator.samples // batch_size)\n",
    "\n",
    "    print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "    print(f\"Validation steps: {validation_steps}\")\n",
    "\n",
    "    # Compile the model with an optimizer, loss function, and metrics\n",
    "    finetuning_model.compile(optimizer=SGD(), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    # Train the model using the training and validation data generators\n",
    "    history = finetuning_model.fit(train_generator,\n",
    "                                   steps_per_epoch=steps_per_epoch,\n",
    "                                   validation_data=validation_generator,\n",
    "                                   validation_steps=validation_steps,\n",
    "                                   epochs=50,\n",
    "                                   callbacks=callbacks,\n",
    "                                   verbose=1,\n",
    "                                   shuffle=True)\n",
    "\n",
    "    # Get predictions and generate confusion matrix\n",
    "#    y_true, y_predict = model_prediction(finetuning_model, learned_weights, save_here)\n",
    "    y_true, y_predict = model_prediction(finetuning_model, learned_weights)\n",
    "\n",
    "    ConfusionMatrix(y_true, y_predict, save_here, folder, index)\n",
    "\n",
    "    return finetuning_model, learned_weights, save_here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80ab413",
   "metadata": {
    "id": "f80ab413"
   },
   "source": [
    "###  Make a prediction on a test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3519af4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:24:03.527154Z",
     "iopub.status.busy": "2023-04-30T12:24:03.526422Z",
     "iopub.status.idle": "2023-04-30T12:24:03.534457Z",
     "shell.execute_reply": "2023-04-30T12:24:03.533373Z",
     "shell.execute_reply.started": "2023-04-30T12:24:03.527114Z"
    },
    "id": "f3519af4"
   },
   "outputs": [],
   "source": [
    "def model_prediction( model, best_learnetweights, batch_size=1):\n",
    "\n",
    "    x_test , y_test = [] , []\n",
    "    for i in range(test_generator.n//1):\n",
    "        a , b = test_generator.next()\n",
    "        x_test.extend(a)\n",
    "        y_test.extend(b)\n",
    "    y_test= np.array(y_test)\n",
    "\n",
    "    # Predict the output\n",
    "    STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
    "    test_generator.reset()\n",
    "\n",
    "\n",
    "    # loading the convergence weights\n",
    "    model.load_weights(best_learnetweights)\n",
    "\n",
    "    #make prediction\n",
    "    print('Make a prediction on a test set:')\n",
    "    y_test_pred= model.predict(test_generator,steps=STEP_SIZE_TEST,verbose=1)\n",
    "    y_prediction = np.argmax(y_test_pred, axis=1)## predicted_class_indices\n",
    "\n",
    "    # ground truth labels\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "    return y_true, y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84688a3c",
   "metadata": {
    "id": "84688a3c"
   },
   "source": [
    "### Evaluate the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a7e03e",
   "metadata": {
    "id": "41a7e03e"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- test_classes: List of class names used in the classification task.\n",
    "- y_predict: Array of predicted labels.\n",
    "- y_true: Array of true labels.\n",
    "- folder_name: Identifier for the current experiment or granularity level.\n",
    "- index: Correction factor applied to predictions when needed.\n",
    "- save_to_dir: Directory where evaluation results will be saved.\n",
    "\"\"\"\n",
    "\n",
    "def ConfusionMatrix (y_true, y_predict, save_here, folder_name, index):       #get confusion matrix\n",
    "\n",
    "    if folder_name !='k_1':        ###  Refine the final classification using error-correction criteria.\n",
    "        correct_prediction=[]\n",
    "        for i in y_predict:\n",
    "            correct_prediction.append(i // index)\n",
    "        y_predict=np.array(correct_prediction)\n",
    "\n",
    "\n",
    "    # get confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_predict)\n",
    "    class_names = [cls for cls, idx in sorted(test_classes.items(), key=lambda x: x[1])]\n",
    "\n",
    "    # plot\n",
    "    fig, ax = plot_confusion_matrix(conf_mat=cm,  figsize=(6, 6),\n",
    "                            colorbar=False,\n",
    "                            show_absolute=True,\n",
    "                            show_normed=False,\n",
    "                            class_names=test_classes,cmap=\"Blues\")\n",
    "    # save the figure   \n",
    "    cm_filename = f'ConfusionMatrix_{folder_name}_iter{index}.png'\n",
    "    plt.savefig(os.path.join(save_here, cm_filename))\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # Classification report\n",
    "    report = classification_report(y_true, y_predict, target_names=class_names, digits=4, output_dict=True)\n",
    "    accuracy = accuracy_score(y_true, y_predict)\n",
    "\n",
    "    # Print summary\n",
    "    print(classification_report(y_true, y_predict, target_names=class_names, digits=4))\n",
    "    print('Overall accuracy= ', accuracy)\n",
    "\n",
    "    # Save confusion matrix values as CSV\n",
    "    #cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "    #cm_csv = f'ConfusionMatrix_{folder_name}_iter{index}.csv'\n",
    "    #cm_df.to_csv(os.path.join(save_here, cm_csv))\n",
    "\n",
    "    # Save classification report as CSV\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    report_csv = f'ClassificationReport_{folder_name}_iter{index}.csv'\n",
    "    report_df.to_csv(os.path.join(save_here, report_csv))\n",
    "\n",
    "   # print(f\"Confusion matrix saved to {cm_csv}\")\n",
    "    #print(f\"Classification report saved to {report_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e493dd9",
   "metadata": {
    "id": "9e493dd9"
   },
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5aaced",
   "metadata": {
    "id": "4a5aaced",
    "outputId": "4501de51-8590-463f-ec23-af4f005fa1ca"
   },
   "outputs": [],
   "source": [
    "# Paths to the dataset and the directory where results will be saved\n",
    "\n",
    "Dataset_path = '................./labelled_dataset/'\n",
    "save_to_dir = '................./results/'\n",
    "pseudo_labels_path= '........../pseudo_labells/'\n",
    "\n",
    "# Determine the decomposition granularity by listing all the folders in the dataset path\n",
    "# Each folder represents a granularity level (e.g., g1, g2, ..., g9_max)\n",
    "decomposition_granularity = os.listdir(Dataset_path)\n",
    "\n",
    "\n",
    "# Sort the granularity levels in descending order (from g_max to g_min)\n",
    "# Example: [g5, g4, ..., g1]\n",
    "decomposition_granularity.sort(reverse=True)\n",
    "\n",
    "# Initialize lists to store the granularity levels (G) and their corresponding indices\n",
    "\n",
    "G=[]\n",
    "index=[]\n",
    "decomposition_granularity.sort(reverse=True)\n",
    "\n",
    "for i, folder in list(enumerate(decomposition_granularity,1)):         # G=[9_max,...,g2,g1]\n",
    "    #print('Index= ',i, '  The granularity decomposition class is: ',folder)\n",
    "    G.append(folder)     # if max=5 then: G=[9_5, g_4, g_3, g_2, g_1]\n",
    "    index.append(i)      # if max=5 then:  index=[5, 4, 3, 2, 1]\n",
    "\n",
    "index.sort(reverse=True)\n",
    "# Display the sorted granularity levels and corresponding indices\n",
    "print ('______________________________________________\\n')\n",
    "print('G = \\n', decomposition_granularity )\n",
    "print('index= ', index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acb0a9c",
   "metadata": {
    "id": "0acb0a9c",
    "outputId": "ae69643a-f6ac-4500-9351-1b29e021ff88",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Select_model = The_proccess()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
